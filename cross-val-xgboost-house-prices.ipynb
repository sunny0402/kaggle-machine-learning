{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "# 0. Create a class to preprocess data.\n",
    "# 1. Use entire training data to determine model accuracy with cross validation and and XGBoost model.\n",
    "# 2. Used optimized model parameters to make predictions on test data.\n",
    "# 3. Submit predictions to Kaggle Competition: https://www.kaggle.com/competitions/home-data-for-ml-course/leaderboard#\n",
    "\n",
    "\n",
    "# Notes:\n",
    "# Train and test data must pass through same preprocessing .... features that train on should be present in test\n",
    "# Save model into JSON format.\n",
    "# clf.save_model(\"clf.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "pd.set_option('display.max_columns',200)\n",
    "pd.set_option('display.max_rows',500)\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model = XGBRegressor()\n",
    "# test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PreprocessData:\n",
    "    \"\"\" \n",
    "        - The PreProcessData class reads a CSV file and processes the data therein. \n",
    "        - PreprocessData can take in either a train or test data set.\n",
    "        - Encoders applied to training data can be passed to test data to apply the same transformations.\n",
    "        - Preprocessing occurs before a train/valid/test split.\n",
    "        - The class returns target (y), predictors (X), and transformer objects\n",
    "        (imputer, ordinal encoder, one-hot encoder) applied to the data.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,imputer=None, ordinal_encoder = None,one_hot_encoder=None, is_train = True, new_feature_names=None):\n",
    "        # target and predictors\n",
    "        self.full_data = None\n",
    "        self.y = None\n",
    "        self.X = None\n",
    "\n",
    "        # columns\n",
    "        self.numerical_cols = []\n",
    "        self.categorical_cols = []\n",
    "        self.ordinal_cols = []\n",
    "\n",
    "        # column transformers\n",
    "        self.imputer = imputer\n",
    "        self.ordinal_encoder = ordinal_encoder\n",
    "        self.one_hot_encoder = one_hot_encoder\n",
    "        \n",
    "\n",
    "        # dataframes\n",
    "        self.imputed_df = None\n",
    "        self.ordinal_encoded_df = None\n",
    "\n",
    "        self.new_feature_names = new_feature_names\n",
    "        self.one_hot_encoded_df = None\n",
    "\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def get_original_data(self):\n",
    "        return self.full_data\n",
    "    \n",
    "    def set_data(self,directory,index_column=None):\n",
    "        self.full_data = pd.read_csv(directory, index_col=index_column)\n",
    "    \n",
    "\n",
    "    def set_X_y(self,target_column_name=None):\n",
    "        if self.is_train and target_column_name:\n",
    "            all_data = self.get_original_data().copy()\n",
    "            # remove rows with no y values\n",
    "            all_data.dropna(axis=0,subset=[target_column_name],inplace=True)\n",
    "            # set target\n",
    "            self.y = all_data[target_column_name]\n",
    "            # drop SalePrice column from predictors\n",
    "            self.X  = all_data.drop([target_column_name],axis=1,inplace=False)\n",
    "        else: #Note: Kaggle test data has no y-values\n",
    "            self.X = self.get_original_data().copy()\n",
    "\n",
    "\n",
    "    def get_y(self):\n",
    "        return self.y\n",
    "    \n",
    "    def get_X(self):\n",
    "        return self.X\n",
    "\n",
    "    def handle_missing_values(self,input_df,strat):\n",
    "        if self.imputer:\n",
    "            self.imputed_df = pd.DataFrame(self.imputer.transform(input_df))\n",
    "            self.imputed_df.columns = input_df.columns\n",
    "            self.imputed_df.index = input_df.index\n",
    "            #preserve data types\n",
    "            self.imputed_df = self.imputed_df.astype(input_df.dtypes.to_dict())\n",
    "            return self.imputed_df\n",
    "        elif self.imputer == None: # Note: no imputer provided thus instantiate\n",
    "            self.imputer = SimpleImputer(strategy=strat)\n",
    "            self.imputed_df = pd.DataFrame(self.imputer.fit_transform(input_df))\n",
    "            self.imputed_df.columns = input_df.columns\n",
    "            self.imputed_df.index = input_df.index\n",
    "            #preserve data types\n",
    "            self.imputed_df = self.imputed_df.astype(input_df.dtypes.to_dict())\n",
    "            return self.imputed_df,self.imputer\n",
    "            \n",
    "\n",
    "    def get_imputed_df(self):\n",
    "        return self.imputed_df\n",
    "    \n",
    "    def get_imputer(self):\n",
    "        return self.imputer\n",
    "    \n",
    "\n",
    "    def set_numerical_columns(self, input_df):\n",
    "        self.numerical_cols =  [ col for col in input_df.columns.values\n",
    "                        if input_df[col].dtype == \"float64\" or \\\n",
    "                            input_df[col].dtype == \"int64\"]\n",
    "\n",
    "    def set_categorical_columns(self,input_df):\n",
    "        self.categorical_cols = [col for col in input_df.columns.values\n",
    "                if input_df[col].dtype == \"object\"]\n",
    "    \n",
    "\n",
    "    def set_ordinal_columns(self, column_list):\n",
    "        self.ordinal_cols = column_list\n",
    "        # if self.get_categorical_columns():\n",
    "        #     self.categorical_cols = [ col for col in self.get_categorical_columns() if col not in column_list]\n",
    "\n",
    "    def get_ordinal_columns(self):\n",
    "        return self.ordinal_cols\n",
    "\n",
    "    def get_numerical_columns(self):\n",
    "        return self.numerical_cols\n",
    "\n",
    "    def get_categorical_columns(self):\n",
    "        return self.categorical_cols\n",
    "    \n",
    "\n",
    "    def get_ordinal_encoder(self):\n",
    "        return self.ordinal_encoder\n",
    "\n",
    "    def encode_ordinal_data(self,input_df):\n",
    "        if self.ordinal_encoder:\n",
    "            self.ordinal_encoded_df = pd.DataFrame(self.ordinal_encoder.transform(input_df))\n",
    "            self.ordinal_encoded_df.index = input_df.index\n",
    "            self.ordinal_encoded_df.columns = input_df.columns\n",
    "            return self.ordinal_encoded_df\n",
    "        elif self.ordinal_encoder == None: #Note: training dataframe\n",
    "            self.ordinal_encoder = OrdinalEncoder()\n",
    "            self.ordinal_encoded_df = pd.DataFrame(self.ordinal_encoder.fit_transform(input_df))\n",
    "            self.ordinal_encoded_df.index = input_df.index\n",
    "            self.ordinal_encoded_df.columns = input_df.columns\n",
    "            return self.ordinal_encoded_df, self.ordinal_encoder\n",
    "\n",
    "    def get_low_high_cardinality_columns(self, input_df,column_list, max_cardinality):\n",
    "        low_card = []\n",
    "        high_card = []\n",
    "        for col in column_list:\n",
    "            if input_df[col].nunique() <= max_cardinality and input_df[col].dtype == \"object\":\n",
    "                low_card.append(col)\n",
    "            else:\n",
    "                high_card.append(col)\n",
    "        return low_card, high_card\n",
    "    \n",
    "\n",
    "\n",
    "    def reduce_cardinality(self,column, threshold = 0.70, return_categories = True):\n",
    "        threshold_value = threshold*len(column)\n",
    "        frequency_sum = 0\n",
    "\n",
    "        new_category_list = []\n",
    "        counts=Counter(column)\n",
    "        most_common = dict(counts.most_common())\n",
    "\n",
    "        for category,count in most_common.items():\n",
    "            frequency_sum = frequency_sum + count\n",
    "            new_category_list.append(category)\n",
    "            if frequency_sum >= threshold_value:\n",
    "                break\n",
    "        # Test\n",
    "        new_category_list.append(\"Other\")\n",
    "        new_column = column.apply(lambda x: x if x in new_category_list else \"Other\")\n",
    "        \n",
    "        if return_categories:\n",
    "            return new_column,new_category_list\n",
    "        else:\n",
    "            return new_column\n",
    "        \n",
    "    def create_low_cardinality_df(self,input_df,low_card_col,high_card_col):\n",
    "        low_card_df = input_df[low_card_col]\n",
    "        for col in high_card_col:\n",
    "            transformed_col = self.reduce_cardinality(input_df[col],threshold=0.7,return_categories=False)\n",
    "            low_card_df = pd.concat([low_card_df,transformed_col.to_frame()],axis=1)\n",
    "    \n",
    "        return low_card_df\n",
    "    \n",
    "\n",
    "    def one_hot_encode(self,input_df):\n",
    "        if self.one_hot_encoder == None: # Note: training data\n",
    "            self.one_hot_encoder = OneHotEncoder(handle_unknown=\"ignore\",sparse_output=False)\n",
    "            self.one_hot_encoded_df = pd.DataFrame(self.one_hot_encoder.fit_transform(input_df))\n",
    "            self.one_hot_encoded_df.index = input_df.index\n",
    "            self.new_feature_names = self.one_hot_encoder.get_feature_names_out(input_df.columns.values)\n",
    "            self.one_hot_encoded_df.columns = self.new_feature_names\n",
    "            self.one_hot_encoded_df.columns = self.one_hot_encoded_df.columns.astype(str)\n",
    "            return self.one_hot_encoded_df,self.one_hot_encoder,self.new_feature_names\n",
    "\n",
    "        elif self.one_hot_encoder != None: #Note: test data\n",
    "            self.one_hot_encoded_df = pd.DataFrame(self.one_hot_encoder.transform(input_df))\n",
    "            self.one_hot_encoded_df.index = input_df.index\n",
    "            self.one_hot_encoded_df.columns = self.new_feature_names\n",
    "            self.one_hot_encoded_df.columns = self.one_hot_encoded_df.columns.astype(str)\n",
    "            return self.one_hot_encoded_df\n",
    "        \n",
    "\n",
    "    # def convert_column_dtypes(self,input_df, current_dtype,new_dtype):\n",
    "    #     columns_to_change = input_df.loc[:,input_df.dtypes==input_dtype]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO test as python script ... if __name__ == \"main\": run the code below\n",
    "\n",
    "# Note: read data and setting X and y\n",
    "process_training_data = PreprocessData()\n",
    "process_training_data.set_data(\"./home-data-for-ml-course/train.csv\",index_column=\"Id\")\n",
    "process_training_data.set_X_y(\"SalePrice\")\n",
    "y_train = process_training_data.get_y()\n",
    "X_train = process_training_data.get_X()\n",
    "\n",
    "\n",
    "\n",
    "# Note: impute missing values\n",
    "imputed_train_df, imputer = process_training_data.handle_missing_values(X_train,strat=\"most_frequent\")\n",
    "\n",
    "# Note: determined examining data ... \n",
    "ordinal_columns = [\"ExterQual\", \"ExterCond\", \"BsmtQual\", \"BsmtCond\",\"BsmtExposure\",\"BsmtFinType1\",\"BsmtFinType2\",\\\n",
    "\"HeatingQC\",\"KitchenQual\",\"Functional\",\"FireplaceQu\",\"GarageQual\",\"GarageCond\"]\n",
    "\n",
    "# Test set and get of numerical, categorical, ordinal columns\n",
    "process_training_data.set_numerical_columns(imputed_train_df)\n",
    "process_training_data.set_categorical_columns(imputed_train_df)\n",
    "process_training_data.set_ordinal_columns(ordinal_columns) \n",
    "\n",
    "\n",
    "# TODO these should be class attributes\n",
    "train_numerical_columns = process_training_data.get_numerical_columns()\n",
    "train_categorical_columns = process_training_data.get_categorical_columns()\n",
    "train_ordinal_columns = process_training_data.get_ordinal_columns()\n",
    "\n",
    "ordinal_encoded_df, ordinal_encoder = \\\n",
    "process_training_data.encode_ordinal_data(imputed_train_df[ordinal_columns])\n",
    "\n",
    "one_hot_columns = [col for col in train_categorical_columns if col not in train_ordinal_columns]\n",
    "\n",
    "low_card, high_card =\\\n",
    "    process_training_data.get_low_high_cardinality_columns(imputed_train_df,one_hot_columns,6)\n",
    "\n",
    "df_to_1hot_encode = process_training_data.\\\n",
    "    create_low_cardinality_df(imputed_train_df,low_card, high_card)\n",
    "\n",
    "\n",
    "one_hot_encoded_df,one_hot_encoder, encoded_features = process_training_data.\\\n",
    "                    one_hot_encode(df_to_1hot_encode)\n",
    "\n",
    "\n",
    "train_dfs_to_combine = [ imputed_train_df[train_numerical_columns],ordinal_encoded_df,one_hot_encoded_df]\n",
    "\n",
    "X_train_df = pd.concat(train_dfs_to_combine,axis=1)\n",
    "\n",
    "\n",
    "# TODO\n",
    "# instantiate process test data after all steps for training data complete ... so can pass imputer, encoder ....\n",
    "# where and when to use previously written valid_categorical_columns() .... \n",
    "# maybe can simple use  handle_unknown=\"use_encoded_value\", unknown_value=10\n",
    "# how to provide custom dictionary to encoder ... maybe must provided the encoded list for column i\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO fit and score function to use XGBoost early stopping, to prevent overfit, and cross-val\n",
    "\n",
    "#Notes: https://xgboost.readthedocs.io/en/latest/python/sklearn_estimator.html\n",
    "# Notes: https://www.kaggle.com/code/robikscube/cross-validation-visualized-youtube-tutorial/notebook\n",
    "# Notes: https://xgboost.readthedocs.io/en/stable/python/python_api.html\n",
    "# \n",
    "\n",
    "def fit_and_score(model,X_train,X_test, y_train,y_test):\n",
    "    model.fit(X_train,y_train,eval_set=[(X_test,y_test)], verbose=False)\n",
    "    r_squared = model.score(X_test,y_test) #coefficient of determination of the prediction\n",
    "    #Note: If early stopping occurs: best_score, best_iteration and best_ntree_limit\n",
    "    return model.best_score, model.best_ntree_limit, r_squared\n",
    "\n",
    "kf = KFold(n_splits=5,shuffle=True,random_state=1)\n",
    "my_model = XGBRegressor(random_state=1,n_estimators=500,\\\n",
    "                        learning_rate=0.05,\\\n",
    "                        early_stopping_rounds=5,\\\n",
    "                        eval_metric=mean_absolute_error, # from sklearn\n",
    "                        # verbosity=0\n",
    "                        )\n",
    "\n",
    "results = {}\n",
    "\n",
    "for fold,(train_idx, val_idx) in enumerate(kf.split(X_train_df,y_train)):\n",
    "\n",
    "    X_tr = X_train_df.iloc[train_idx]\n",
    "    y_tr = y_train.iloc[train_idx]\n",
    "\n",
    "    X_val = X_train_df.iloc[val_idx]\n",
    "    y_val = y_train.iloc[val_idx]\n",
    "    \n",
    "    best_score, best_num_trees, r2 = fit_and_score(clone(my_model),X_tr,X_val,y_tr,y_val)\n",
    "    results[best_num_trees] = [best_score, r2]\n",
    "\n",
    "    \n",
    "\n",
    "#print(results)\n",
    "\n",
    "    # Note: fit model on training data\n",
    "    # my_model = XGBRegressor(random_state=0,n_estimators=100,learning_rate=0.1)\n",
    "    #my_model.fit(X_train_df,y_train)\n",
    "    # Note: make predictions on validation data\n",
    "    #prediction = my_model.predict(X_val)\n",
    "    #print(\"Fold: \",fold, \"Mean Absolute Error: \", mean_absolute_error(prediction,y_val))\n",
    "\n",
    "\n",
    "    # Test\n",
    "    #my_model = XGBRegressor(random_state=0,n_estimators=500,learning_rate=0.05)\n",
    "    #my_model.fit(X_tr,y_tr,eval_set=[(X_val,y_val)],early_stopping_rounds=5,verbose=False)\n",
    "    #train_score = my_model.score(X_tr,y_tr)\n",
    "    #valid_score = my_model.score(X_val,y_val)\n",
    "    #print(train_score,valid_score)\n",
    "    # Test\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{171: [16122.106445, 0.8444595581132778],\n",
       " 145: [16563.832031, 0.8964752968373668],\n",
       " 151: [17535.564453, 0.8662301253799178],\n",
       " 143: [17532.054688, 0.8191237436112073],\n",
       " 170: [13657.378906, 0.925793041530367]}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1459, 170) 1459\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# 1. Preprocess test data used for competition: https://www.kaggle.com/competitions/home-data-for-ml-course/leaderboard#\n",
    "# 2. With optimal hyperparemeters determined with train/val data, fit model on all training data\n",
    "# 3. Generate predictions for competition on test data using final model\n",
    "\n",
    "# Note: initialize class with column transformers used on training data\n",
    "process_test_data = PreprocessData(imputer=imputer,ordinal_encoder=ordinal_encoder,\\\n",
    "                                   one_hot_encoder=one_hot_encoder,\\\n",
    "                                   new_feature_names = encoded_features,\\\n",
    "                                    is_train=False)\n",
    "process_test_data.set_data(\"./home-data-for-ml-course/test.csv\",index_column=\"Id\")\n",
    "process_test_data.set_X_y()\n",
    "\n",
    "X_test = process_test_data.get_X()\n",
    "\n",
    "#print(X_test.shape)\n",
    "\n",
    "imputed_test_df = process_test_data.handle_missing_values(X_test,strat=\"most_frequent\")\n",
    "\n",
    "#print(imputed_test_df.shape)\n",
    "\n",
    "# Note: determined by examining data ... \n",
    "ordinal_columns = [\"ExterQual\", \"ExterCond\", \"BsmtQual\", \"BsmtCond\",\"BsmtExposure\",\"BsmtFinType1\",\"BsmtFinType2\",\\\n",
    "\"HeatingQC\",\"KitchenQual\",\"Functional\",\"FireplaceQu\",\"GarageQual\",\"GarageCond\"]\n",
    "\n",
    "# Note: set and get numerical, categorical, ordinal columns\n",
    "process_test_data.set_numerical_columns(imputed_test_df)\n",
    "process_test_data.set_categorical_columns(imputed_test_df)\n",
    "process_test_data.set_ordinal_columns(ordinal_columns) \n",
    "\n",
    "test_numerical_columns = process_test_data.get_numerical_columns()\n",
    "test_categorical_columns = process_test_data.get_categorical_columns()\n",
    "test_ordinal_columns = process_test_data.get_ordinal_columns()\n",
    "\n",
    "ordinal_encoded_test_df = \\\n",
    "process_training_data.encode_ordinal_data(imputed_test_df[ordinal_columns])\n",
    "\n",
    "#print(ordinal_encoded_test_df.shape)\n",
    "\n",
    "one_hot_test_cols = [col for col in test_categorical_columns if col not in test_ordinal_columns]\n",
    "\n",
    "low_c_test, high_c_test =\\\n",
    "    process_training_data.get_low_high_cardinality_columns(imputed_train_df,one_hot_columns,6)\n",
    "\n",
    "test_df_to_1hot_encode = process_test_data.\\\n",
    "    create_low_cardinality_df(imputed_test_df,low_c_test, high_c_test)\n",
    "\n",
    "\n",
    "one_hot_encoded_test_df = process_training_data.\\\n",
    "                    one_hot_encode(test_df_to_1hot_encode)\n",
    "\n",
    "\n",
    "test_dfs_to_combine = [ imputed_test_df[test_numerical_columns],ordinal_encoded_test_df,one_hot_encoded_test_df]\n",
    "\n",
    "X_test_df = pd.concat(test_dfs_to_combine,axis=1)\n",
    "\n",
    "# 2. With optimal hyperparameters determined with train/val data, fit model on all training data\n",
    "my_competition_model = XGBRegressor(random_state=1,n_estimators=170,learning_rate=0.05)\n",
    "my_competition_model.fit(X_train_df,y_train)\n",
    "\n",
    "# 3. Generate predictions for competition on test data using final model\n",
    "competition_predictions = my_competition_model.predict(X_test_df)\n",
    "\n",
    "print(X_test_df.shape, len(competition_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test predictions to file for Kaggle competition submission: https://www.kaggle.com/c/home-data-for-ml-course\n",
    "output = pd.DataFrame({'Id': X_test_df.index,\n",
    "                       'SalePrice': competition_predictions})\n",
    "output.to_csv('submission_cross_val_xgboos_early_stopt.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "explore_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16 | packaged by conda-forge | (main, Feb  1 2023, 21:42:20) \n[Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61ee244c42315ecc49a37ad94395d343cf26155652760a1aec32be1e60ecf717"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
