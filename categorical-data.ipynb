{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Categorical Data\n",
    "\n",
    "- Notebook based on Kaggle Tutorial:\n",
    "-  https://www.kaggle.com/code/alexisbcook/categorical-variables\n",
    "-  https://www.kaggle.com/learn/intermediate-machine-learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "# 1. Split features and target. And generate train/test split for training data.\n",
    "# 2. Impute missing values. \n",
    "# 3. Determine categorical columns and numerical columns.\n",
    "# 4. Encode categorical columns which are ordinal and have low cardinality.\n",
    "#     Ensure consistent column values between train/validation.\n",
    "# 5. Reduce cardinality by introducing \"Other\" category for low frequency categories.\n",
    "# 6. One-hot encode the remainder of the categorical variables, after reducing their cardinality.\n",
    "# 7. Combine numerical and encoded categorical variables.\n",
    "# 8. Create a random forest model. Fit to train data and check model accuracy on validation data.\n",
    "# 9. categorical-data-competition-prediction.ipynb:  \n",
    "# Preprocess final test data, use all of training data for model fit, generate predictions and submit to competition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Split features and target. And generate train/test split for training data.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns',100)\n",
    "pd.set_option('display.max_rows',200)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_full = pd.read_csv(\"./home-data-for-ml-course/train.csv\",index_col=\"Id\")\n",
    "X_test_full = pd.read_csv(\"./home-data-for-ml-course/test.csv\",index_col=\"Id\")\n",
    "\n",
    "# remove rows with no y values\n",
    "X_full.dropna(axis=0,subset=['SalePrice'],inplace=True)\n",
    "\n",
    "y = X_full['SalePrice']\n",
    "\n",
    "# drop SalePrice column from predictors\n",
    "X_full.drop(['SalePrice'],axis=1,inplace=True)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_full,y,train_size=0.8,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Impute missing values. \n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "imputed_X_train = pd.DataFrame(imputer.fit_transform(X_train))\n",
    "imputed_X_valid = pd.DataFrame(imputer.transform(X_valid))\n",
    "\n",
    "# Fill in the lines below: imputation removed column names, put them back\n",
    "imputed_X_train.columns = X_train.columns\n",
    "imputed_X_valid.columns = X_valid.columns\n",
    "\n",
    "# Test: preserve index of X_train, X_valid dataset ...\n",
    "imputed_X_train.index = X_train.index\n",
    "imputed_X_valid.index = X_valid.index\n",
    "\n",
    "#preserve data types\n",
    "imputed_X_train = imputed_X_train.astype(X_train.dtypes.to_dict())\n",
    "imputed_X_valid = imputed_X_valid.astype(X_train.dtypes.to_dict())\n",
    "\n",
    "\n",
    "print(imputed_X_train.dtypes.unique())\n",
    "print(len(imputed_X_train.columns))\n",
    "print(len([col for col in imputed_X_train.columns if imputed_X_train[col].dtype == \"object\"]))\n",
    "print(len([col for col in imputed_X_train.columns if imputed_X_train[col].dtype == \"int64\"]))\n",
    "print(len([col for col in imputed_X_train.columns if imputed_X_train[col].dtype == \"float64\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Determine categorical columns and numerical columns.\n",
    "object_cols = [col for col in imputed_X_train.columns if imputed_X_train[col].dtype == \"object\"]\n",
    "numerical_cols = [col for col in imputed_X_train.columns \n",
    "                    if imputed_X_train[col].dtype == \"int64\" or \\\n",
    "                         imputed_X_train[col].dtype == \"float64\" ]\n",
    "\n",
    "numerical_X_train_df = imputed_X_train[numerical_cols]\n",
    "numerical_X_valid_df = imputed_X_valid[numerical_cols]\n",
    "\n",
    "print(len(object_cols))\n",
    "print(len(numerical_cols))\n",
    "print(len(imputed_X_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Encode categorical columns which are ordinal. Ensure consistent column values between train/validation.\n",
    "\n",
    "# Note: determine ordinal columns by looking at data_description.txt\n",
    "ordinal_cols = [\"ExterQual\", \"ExterCond\", \"BsmtQual\", \"BsmtCond\",\"BsmtExposure\",\"BsmtFinType1\",\"BsmtFinType2\",\\\n",
    "\"HeatingQC\",\"KitchenQual\",\"Functional\",\"FireplaceQu\",\"GarageQual\",\"GarageCond\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: columns which have consistent values between training and validation set thus can be safely encoded\n",
    "\n",
    "good_label_ordinal_cols = [col for col in ordinal_cols\n",
    "                   if set(imputed_X_valid[col]).issubset(set(imputed_X_train[col]))]\n",
    "bad_label_ordinal_cols = list(set(ordinal_cols) - set(good_label_ordinal_cols))\n",
    "\n",
    "print('Categorical columns that will be ordinal encoded:', good_label_ordinal_cols)\n",
    "print('\\nCategorical columns that will be dropped from the dataset:', bad_label_ordinal_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_object_cols = list(set(object_cols) - (set(good_label_ordinal_cols+bad_label_ordinal_cols))) \n",
    "print(len(object_cols))\n",
    "print(len(good_label_ordinal_cols))\n",
    "print(len(remaining_object_cols)) #Note: #bad_label_cols dropped ... fix column values or handle by one-hot encoder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_X_train = imputed_X_train.copy()\n",
    "ordinal_X_valid = imputed_X_valid.copy()\n",
    "\n",
    "ordinal_X_train = ordinal_X_train[good_label_ordinal_cols]\n",
    "ordinal_X_valid = ordinal_X_valid[good_label_ordinal_cols]\n",
    "\n",
    "# Apply ordinal encoder to categorical cols with good labels\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "ordinal_encoded_X_train = pd.DataFrame(ordinal_encoder.fit_transform(ordinal_X_train))\n",
    "ordinal_encoded_X_valid = pd.DataFrame(ordinal_encoder.transform(ordinal_X_valid))\n",
    "\n",
    "ordinal_encoded_X_train.index = imputed_X_train.index\n",
    "ordinal_encoded_X_valid.index = imputed_X_valid.index\n",
    "\n",
    "ordinal_encoded_X_train.columns = ordinal_X_train.columns\n",
    "ordinal_encoded_X_valid.columns = ordinal_X_valid.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(ordinal_encoded_X_train))\n",
    "print(ordinal_encoded_X_train.shape)\n",
    "print(ordinal_encoded_X_valid.shape)\n",
    "\n",
    "print(ordinal_encoded_X_train.index)\n",
    "print(ordinal_encoded_X_train.columns)\n",
    "\n",
    "ordinal_encoded_X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoded_X_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: One-hot encode the remaining object columns\n",
    "print(remaining_object_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Reduce cardinality by introducing \"Other\" category for low frequency categories.\n",
    "low_cardinality_cols = []\n",
    "high_cardinality_cols = []\n",
    "for col in remaining_object_cols:\n",
    "    if imputed_X_train[col].nunique() < 8 and imputed_X_train[col].dtype == \"object\":\n",
    "        low_cardinality_cols.append(col)\n",
    "    else:\n",
    "        high_cardinality_cols.append(col)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Can I reduce the cardinality of high cardinality columns by using an \"Other\" label for low frequency values\n",
    "# Based on: https://towardsdatascience.com/dealing-with-features-that-have-high-cardinality-1c9212d7ff1b\n",
    "\n",
    "from collections import Counter\n",
    "def reduce_cardinality(column, threshold = 0.75, return_categories = True):\n",
    "    # print(\"Input cardinality: \",column.nunique())\n",
    "    threshold_value = threshold*len(column)\n",
    "    frequency_sum = 0\n",
    "\n",
    "    new_category_list = []\n",
    "    counts=Counter(column)\n",
    "    most_common = dict(counts.most_common())\n",
    "\n",
    "    for category,count in most_common.items():\n",
    "        frequency_sum = frequency_sum + count\n",
    "        new_category_list.append(category)\n",
    "        if frequency_sum >= threshold_value:\n",
    "            break\n",
    "    # Test\n",
    "    new_category_list.append(\"Other\")\n",
    "    new_column = column.apply(lambda x: x if x in new_category_list else \"Other\")\n",
    "    # print(\"Output cardinality: \",new_column.nunique())\n",
    "    # print(\"new index: \\n\",new_column.index)\n",
    "    # print(\"new column name: \\n\",new_column.name)\n",
    "    \n",
    "    if return_categories:\n",
    "        return new_column,new_category_list\n",
    "    else:\n",
    "        return new_column\n",
    "\n",
    "\n",
    "transformed_col, new_categories = reduce_cardinality(imputed_X_train[\"Neighborhood\"])\n",
    "print(new_categories)\n",
    "print(type(transformed_col))\n",
    "print(transformed_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OH_X_train = imputed_X_train[low_cardinality_cols]\n",
    "OH_X_valid = imputed_X_valid[low_cardinality_cols]\n",
    "\n",
    "for col in high_cardinality_cols:\n",
    "    transformed_X_train_col = reduce_cardinality(imputed_X_train[col],threshold=0.70, return_categories=False)\n",
    "    OH_X_train = pd.concat([OH_X_train,transformed_X_train_col.to_frame()],axis=1)\n",
    "    # OH_X_train[col] = transformed_X_train_col\n",
    "    \n",
    "    transformed_X_valid_col = reduce_cardinality(imputed_X_valid[col], threshold=0.70, return_categories=False)\n",
    "    OH_X_valid = pd.concat([OH_X_valid,transformed_X_valid_col.to_frame()],axis=1)\n",
    "    # OH_X_valid[col] = transformed_X_valid_col\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(OH_X_train))\n",
    "print(OH_X_train.shape)\n",
    "OH_X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(OH_X_valid))\n",
    "print(OH_X_valid.shape)\n",
    "OH_X_valid.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. One-hot encode the remainder of the categorical variables, after reducing their cardinality.\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Note: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
    "# def custom_combiner(feature, category):\n",
    "#     return str(feature) + \"_\" + type(category).__name__ + \"_\" + str(category)\n",
    "\n",
    "# Note: handle_unknown='ignore': avoid errors when the validation data contains classes that aren't represented in the training data\n",
    "#parse=False:  ensures that the encoded columns are returned as a numpy array (instead of a sparse matrix).\n",
    "hot1_encoder = OneHotEncoder(handle_unknown=\"ignore\",sparse_output=False)\n",
    "\n",
    "OH_encoded_X_train = pd.DataFrame(hot1_encoder.fit_transform(OH_X_train))\n",
    "OH_encoded_X_valid = pd.DataFrame(hot1_encoder.transform(OH_X_valid))\n",
    "\n",
    "OH_encoded_X_train.index = OH_X_train.index\n",
    "OH_encoded_X_valid.index = OH_X_valid.index\n",
    "\n",
    "OH_encoded_X_train.columns = hot1_encoder.get_feature_names_out( OH_X_train.columns.values)\n",
    "OH_encoded_X_valid.columns = hot1_encoder.get_feature_names_out(OH_X_valid.columns.values)\n",
    "\n",
    "print(OH_X_train.shape)\n",
    "print(OH_encoded_X_train.shape)\n",
    "print(\"*\"*10)\n",
    "\n",
    "print(OH_encoded_X_train.index)\n",
    "print(OH_encoded_X_train.columns)\n",
    "print(\"*\"*10)\n",
    "print(OH_encoded_X_valid.index)\n",
    "print(OH_encoded_X_valid.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OH_encoded_X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OH_encoded_X_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Combine numerical and encoded categorical dataframes.\n",
    "train_frames = [numerical_X_train_df, ordinal_encoded_X_train,  OH_encoded_X_train]\n",
    "valid_frames = [numerical_X_valid_df,ordinal_encoded_X_valid, OH_encoded_X_valid ]\n",
    "\n",
    "for df in train_frames:\n",
    "    print(len(df.columns.values))\n",
    "\n",
    "\n",
    "# Note: should not be overlapping columns, but indexes should match\n",
    "# for f in range(len(train_frames)-1):\n",
    "#     intersect = set(train_frames[f].columns.values).intersection(set(train_frames[f+1].columns.values))\n",
    "#     if intersect:\n",
    "#         print(intersect)\n",
    "\n",
    "\n",
    "# for f in range(len(valid_frames)-1):\n",
    "#     intersect = set(valid_frames[f].columns.values).intersection(set(valid_frames[f+1].columns.values))\n",
    "#     if intersect:\n",
    "#         print(intersect)\n",
    "\n",
    "final_X_train = pd.concat(train_frames,axis=1)\n",
    "final_X_valid = pd.concat(valid_frames,axis=1)\n",
    "\n",
    "# print(final_X_train.columns)\n",
    "# print(final_X_valid.columns)\n",
    "\n",
    "print(final_X_train.shape, y_train.shape, final_X_valid.shape,y_valid.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Create a random forest model. Fit to train data and check model accuracy on validation data.\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def check_model_accuracy(X_train,X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators=100,random_state=1)\n",
    "    model.fit(X_train,y_train)\n",
    "    predictions = model.predict(X_valid)\n",
    "    return model, mean_absolute_error(y_valid,predictions)\n",
    "\n",
    "\n",
    "final_model,error = check_model_accuracy(final_X_train, final_X_valid,y_train,y_valid)\n",
    "print(final_model)\n",
    "print(error)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess test data\n",
    "test_imputer = SimpleImputer(strategy='most_frequent')\n",
    "test_ordinal_encoder = OrdinalEncoder()\n",
    "test_1hot_encoder = OneHotEncoder(handle_unknown=\"ignore\",sparse_output=False)\n",
    "\n",
    "#1. Get test data\n",
    "X_test_final = X_test_full.copy()\n",
    "# 2. Impute missing values. \n",
    "X_test_final_imputed = pd.DataFrame(test_imputer.fit_transform(X_test_final))\n",
    "X_test_final_imputed.index = X_test_final.index\n",
    "X_test_final_imputed.columns = X_test_final.columns\n",
    "X_test_final_imputed = X_test_final_imputed.astype(X_test_final.dtypes.to_dict())\n",
    "\n",
    "print(X_test_final_imputed.dtypes.unique())\n",
    "print(X_test_final_imputed.shape)\n",
    "X_test_final_imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Determine categorical and numerical columns.\n",
    "test_object_cols = [col for col in X_test_final_imputed.columns.values\n",
    "                        if X_test_final_imputed[col].dtype == \"object\"]\n",
    "test_numerical_cols = [ col for col in X_test_final_imputed.columns.values\n",
    "                       if X_test_final_imputed[col].dtype == \"float64\" or \\\n",
    "                         X_test_final_imputed[col].dtype == \"int64\"]\n",
    "\n",
    "test_ordinal_cols = [\"ExterQual\", \"ExterCond\", \"BsmtQual\", \"BsmtCond\",\"BsmtExposure\",\"BsmtFinType1\",\"BsmtFinType2\",\\\n",
    "\"HeatingQC\",\"KitchenQual\",\"Functional\",\"FireplaceQu\",\"GarageQual\",\"GarageCond\"]\n",
    "X_train_full = pd.concat([final_X_train, final_X_valid],axis=0)\n",
    "\n",
    "# Note: features used in training data should match those in test data\n",
    "test_data_cols = test_object_cols + test_numerical_cols + test_ordinal_cols\n",
    "\n",
    "for col in X_train_full.columns.values:\n",
    "    if col not in test_data_cols:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Encode categorical columns which are ordinal and have low cardinality.\n",
    "#     Ensure consistent column values between \n",
    "# training data used to fit the model and test data on which generate final predictions for competition.\n",
    "\n",
    "# good_label_ordinal_cols = [col for col in ordinal_cols\n",
    "#                    if set(imputed_X_valid[col]).issubset(set(imputed_X_train[col]))]\n",
    "# bad_label_ordinal_cols = list(set(ordinal_cols) - set(good_label_ordinal_cols))\n",
    "# remaining_object_cols = list(set(object_cols) - (set(good_label_ordinal_cols+bad_label_ordinal_cols))) \n",
    "\n",
    "columns_with_consistent_values = []\n",
    "not_consistent_cols = []\n",
    "\n",
    "X_test_final_imputed.drop()\n",
    "\n",
    "for col in test_ordinal_cols:\n",
    "    # Note: features in test data should match features in full training data set\n",
    "    # Also categories within a feature used to train/fit the model should be consistent with those in test data\n",
    "    if set(X_test_final_imputed[col]).issubset(set(X_train_full[col])):\n",
    "        # print(set(X_final_imputed[col]))\n",
    "        columns_with_consistent_values.append(col)\n",
    "    else:\n",
    "        not_consistent_cols.append(col)\n",
    "\n",
    "\n",
    "# Note which columns can safely one-hot encode?\n",
    "print(\"columns_with_consistent_values: \\n\",  columns_with_consistent_values)  \n",
    "print(\"not_consistent_cols: \\n \",not_consistent_cols)\n",
    "\n",
    "ordinal_X_test = X_test_final_imputed[columns_with_consistent_values]\n",
    "ordinal_encoded_X_test =  pd.DataFrame(test_ordinal_encoder.fit_transform(ordinal_X_test))\n",
    "\n",
    "ordinal_encoded_X_test.index = X_test_final_imputed.index\n",
    "ordinal_encoded_X_test.columns = columns_with_consistent_values\n",
    "\n",
    "print(ordinal_encoded_X_test.shape)\n",
    "ordinal_encoded_X_test.head()\n",
    "                 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Reduce cardinality by introducing \"Other\" category for low frequency categories.\n",
    "remaining_categorical_cols = list(set(test_object_cols) - set(ordinal_X_test.columns.values))     \n",
    "test_low_card = []\n",
    "test_high_card = []\n",
    "for col in remaining_categorical_cols:\n",
    "    if X_test_final_imputed[col].nunique() < 8 and X_test_final_imputed[col].dtype == \"object\":\n",
    "        test_low_card.append(col)\n",
    "    else:\n",
    "        test_high_card.append(col)\n",
    "\n",
    "OH_X_test = X_test_final_imputed[test_low_card]\n",
    "for col in test_high_card:\n",
    "    transformed_X_test_col = reduce_cardinality(X_test_final_imputed[col], threshold=0.7, return_categories=False)\n",
    "    OH_X_test = pd.concat([OH_X_test,transformed_X_test_col.to_frame()],axis=1 )\n",
    "\n",
    "\n",
    "print(OH_X_test.shape)\n",
    "OH_X_test.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. One-hot encode the remainder of the categorical variables, after reducing their cardinality.\n",
    "OH_encoded_X_final = pd.DataFrame(test_1hot_encoder.fit_transform(OH_X_test))\n",
    "\n",
    "OH_encoded_X_final.index = OH_X_test.index\n",
    "OH_encoded_X_final.columns = test_1hot_encoder.get_feature_names_out(OH_X_test.columns.values)\n",
    "print(OH_encoded_X_final.shape)\n",
    "OH_encoded_X_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Combine numerical and encoded categorical variables.\n",
    "test_frames_to_combine = [X_test_final_imputed[test_numerical_cols],ordinal_encoded_X_test,OH_encoded_X_final]\n",
    "for df in test_frames_to_combine:\n",
    "    print(len(df.columns.values))\n",
    "X_test_final_df = pd.concat(test_frames_to_combine, axis=1)\n",
    "print(X_test_final_df.shape)\n",
    "X_test_final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: use all of training data now to train model\n",
    "print(set(final_X_train.columns.values) == set(final_X_valid.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Create a random forest model. Fit to entire training data and check model accuracy on validation data.\n",
    "final_preprocessed_training_data_X = pd.concat([final_X_train,final_X_valid],axis=0)\n",
    "final_preprocessed_training_data_y = pd.concat([y_train,y_valid],axis=0)\n",
    "\n",
    "# Note: number of features used to fit model should equal number of features used to test\n",
    "print(\"number features training: \",len(final_preprocessed_training_data_X.columns))\n",
    "print(\"number features test: \",len( X_test_final_df.columns ))\n",
    "print(set(final_preprocessed_training_data_X.columns) == set(X_test_final_df.columns))\n",
    "\n",
    "\n",
    "competition_model = RandomForestRegressor(n_estimators=100, random_state=1)\n",
    "competition_model.fit(final_preprocessed_training_data_X,final_preprocessed_training_data_y )\n",
    "competition_predictions = competition_model.predict(X_test_final_df)\n",
    "final_predictions = final_model.predict(X_test_final_df)\n",
    "final_predictions\n",
    "# 9. Preprocess final test data, generate predictions and submit to competition. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- For tree-based models (like decision trees and random forests), you can expect ordinal encoding to work well with ordinal variables.\n",
    "- categorical variables without ordering --> nominal variables --> one-hot encoding\n",
    "- one-hot encoding NOT used for a variables which takes more than 15 values\n",
    "- \"Cardinality\" means the number of unique values in a column\n",
    "- One-hot encode low cardinality columns, if can ordinal encode the remainder of categorical columns, drop remainder\n",
    "  \n",
    "```python\n",
    "# low cardinality\n",
    "low_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and X_train_full[cname].dtype == \"object\"]\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Keep selected columns only\n",
    "my_cols = low_cardinality_cols + numerical_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()\n",
    "\n",
    "```\n",
    "\n",
    "#### Ordinal Encoder\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Make copy to avoid changing original data \n",
    "label_X_train = X_train.copy()\n",
    "label_X_valid = X_valid.copy()\n",
    "\n",
    "# Apply ordinal encoder to each column with categorical data\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "label_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])\n",
    "label_X_valid[object_cols] = ordinal_encoder.transform(X_valid[object_cols])\n",
    "```\n",
    "\n",
    "\n",
    "#### One-hot encoder\n",
    "\n",
    "- set handle_unknown='ignore' to avoid errors when the validation data contains classes that aren't represented in the training data\n",
    "- sparse=False ensures that the encoded columns are returned as a numpy array (instead of a sparse matrix).\n",
    "- \n",
    "```python\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Apply one-hot encoder to each column with categorical data\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\n",
    "OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\n",
    "\n",
    "# One-hot encoding removed index; put it back\n",
    "OH_cols_train.index = X_train.index\n",
    "OH_cols_valid.index = X_valid.index\n",
    "\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "num_X_train = X_train.drop(object_cols, axis=1)\n",
    "num_X_valid = X_valid.drop(object_cols, axis=1)\n",
    "\n",
    "# Add one-hot encoded columns to numerical features\n",
    "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
    "\n",
    "# Ensure all columns have string type\n",
    "OH_X_train.columns = OH_X_train.columns.astype(str)\n",
    "OH_X_valid.columns = OH_X_valid.columns.astype(str)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "explore_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61ee244c42315ecc49a37ad94395d343cf26155652760a1aec32be1e60ecf717"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
